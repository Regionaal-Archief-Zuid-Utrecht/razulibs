{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d5b535",
   "metadata": {},
   "source": [
    "# razulibs/razu documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42643f67",
   "metadata": {},
   "source": [
    "### Class ```S3Storage```\n",
    "\n",
    "This class uses boto3, the official Python SDK for AWS to interact with the S3 storage. It wraps aws api calls into class methods, adding needed parameters, exception handling and whatever other additional features are needed. It provides methods for interacting with an S3-compatible storage service. \n",
    "\n",
    "The subclass EDepot, combines these S3 'basic' services to implement ad hoc e-depot pipelines. \n",
    "\n",
    "It includes functionalities to:\n",
    "\n",
    "- Check or create a bucket\n",
    "- Upload files with metadata\n",
    "- List objects in a bucket\n",
    "- Retrieve file metadata\n",
    "- Verify the integrity of uploads using MD5 checksums\n",
    "- Update and retrieve access control lists (ACLs) of objects\n",
    "- Retrieve the bucket policy\n",
    "- Check and retrieve block public access settings\n",
    "\n",
    "The class initializes an S3 client using credentials (endpoint, access key, and secret key) retrieved from environment variables. The credentials should be stored in a `.env` file. Lookup order for `.env` is:\n",
    "\n",
    "1) The current working directory (so different projects/buckets can use their own creds)\n",
    "2) Fallback to the module directory (this file's directory)\n",
    "\n",
    "NOTE: On my client \"sudo hwclock -s\" is sometimes required (when clock is 'skewed')\n",
    "\n",
    "N.B. When passing the object key as argument, it is always \"nl-wbdrazu/k50907905/689/001/067/nl-wbdrazu-k50907905-689-1067806.extension\", not just the last part.\n",
    "\n",
    "\n",
    "**Attributes**\n",
    "\n",
    "Attributes are loaded from an .env file that is accessed following this look up order: first from the current working directory, then fallback to the module directory.\n",
    "\n",
    "```endpoint```: the s3 storage endpoint\n",
    "\n",
    "```access_key```: s3 access key\n",
    "\n",
    "```secret_key```: s3 secret key\n",
    "\n",
    "```s3_client```: the boto3 s3 client instance. This is initialized using the endpoint, access key, and secret key. [boto3 credential](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)\n",
    "\n",
    "\n",
    "**Issues**\n",
    "\n",
    "- env vars stored as attributes [Identity and Access Management in AWS EC2](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html)\n",
    "- we want an error if the env file is not loaded correctly, missing etc.\n",
    "\n",
    "\n",
    "**Methods**\n",
    "\n",
    "```check_or_create_bucket```: Checks if a bucket exists in the S3 storage and creates it if it does not exist. Takes in input a bucket name and a boolean to enamble versioning. True if the bucket exists or was created successfully, False if an error occurred.\n",
    "\n",
    "*this is two tasks and when true is returned you don't know if bucket existed or was created. There's a message printed, so maybe this is not a problem. I guess the logic here is: I want to make a bucket, I don't need to know if it exists already but the back end needs to make sure if the bucket already exists it is not created again.**I wonder what is the use of this function? I want to create a bucket, then maybe if the bucket already exisats we want to return something else than ture, like 'choose another name etc'**.*\n",
    "\n",
    "*Also Why is enable_bucket_versioning False by default? If versioning is set as True, the function calls another method 'set_bucket_versioning' wich is by default set as 'Enabled' status. But **There is no confirmation of setting the versioning**.*\n",
    "\n",
    "*When creating bucket make sure to check naming rules*\n",
    "\n",
    "```set_bucket_versioning```: Sets the versioning status on an S3 bucket. Takes in input the name of the bucket an a string specifying the versioning status ('Enabled'-default or 'Suspended'). Returns True if versioning status was set successfully, False otherwise.\n",
    "\n",
    "*uses [boto3 put_bucjet_versioning method](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_bucket_versioning.html)* \n",
    "\n",
    "```get_bucket_versioning```: Gets the versioning status of an S3 bucket. Takes in input the name of the bucket to check and returns the verisioning status.\n",
    "\n",
    "```store_file```: Uploads one file to the specified S3 bucket along with its metadata. Takes in input the name of the bucket to upload to, the name of the file (objeck_key), the local path of the file to upload and a metadata dictionary. It doesn't return anything but prints out a confirmation text. \n",
    "\n",
    "*Takes the type of file (format) via mimetypes. I do not understand where the metadata in the parameters come from, what are they? I tuses boto3 [upload_file](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html) method*\n",
    "\n",
    "```get_file_metadata```: Returns the metadata of a specific file (object) from an S3 bucket or None if file or bucket don't exist. Take sin input the bucket name the file name (file_key). Uses [boto3 head_object method](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/head_object.html).N.B. This only selewcts the Metadata key of the head_boject response.\n",
    "\n",
    "*the name file_key, should maybe be object_key, sine in the previous method it is called like that. We should harmonize variable names. Why this logic of hiding the error?  '# For missing objects or buckets, return None silently so callers can treat it as \"does not exist\".' Additionally, for the moment I always got errors and could't retrieve metadata*\n",
    "\n",
    "```verify_upload```: Takes in input the bucket name, the file_key (file name, object_key) and the md5 of the file. Returns nothing but prints out a confirmation text of success of failure.\n",
    "\n",
    "*calculates the checksum of the s3 file internally, how is that computationally? Look into computingthe combined MD5 from parts without downloading.***downloading costs money?** \n",
    "*ETag is an opaque identifier assigned by a web server to a specific version of a resource found at a URL. For single uploads, it is usually the md5 checksum. For bigger files, different md5 are computed for each part. that is why the functions tries to capture that and if it's the case downloads the file and recalculate md5. [s3 data integrity](https://repost.aws/knowledge-center/data-integrity-s3) this might be useful, to read. The etag might not necessarily be the md5* \n",
    "\n",
    "```update_acl```: Updates the ACL (Access Control List) for a specific object in the S3 bucket. Returns nothing but prints out a success/error message. \n",
    "\n",
    "*The function uses by default [Canned ACL](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#canned-acl) set to 'public-read' and not an AccessControlList dictionary. It uses the [boto3 put_object_acl](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_object_acl.html) method. To know more about what different control levels entail see [s3 ACL overview](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html). The default 'public-read'means that ALL USERS get READ permission (s3:ListBucket, s3:ListBucketVersions, and s3:ListBucketMultipartUploads\ts3:GetObject and s3:GetObjectVersion), and OWNER gets FULL_CONTROL (the combination of all permissions READ,. WRITE, READ_ACP, WRITE_ACP)|* \n",
    "\n",
    "```get_object_acl```: Takes in input the bucket name and the file_key (object_key) and returns nothing but prints out the ACL of objects.\n",
    "\n",
    "```get_bucket contents```: returns a list of all object keys in a bucket using pagination. Take sin input the bucket name and a 'prefix' which is a starting string of an object key (ex. nl-wbdrazu/k50907905/689 (to check), or NL-WbDRAZU-K50907905-500).\n",
    "\n",
    "Pagination means that results are read by the 'list_object_v2' API in batches of 1000. each page = 1000 objects. The get_bucket_content method reads the pages and append all result to a uniqye list\n",
    "\n",
    "*can take a long time, maybe we should add a status bar.*\n",
    "\n",
    "```get_bucket_policy```: Returns the policy of a specific s3 bucket specified in input, or an error message. Uses boto3 [get_bucket_policy](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/get_bucket_policy.html).\n",
    "\n",
    "*the k50907905 bucket does't have one*\n",
    "\n",
    "```get_block_public_access```:Checks if Block Public Access is enabled for a specific S3 bucket. [Block Public Access](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html) is a feature that sets a centralized control to limit public access. If enable it overrides ACL settings and other set policies to always prevent public access. Returns the block bublic access configuration for the bucket specified in input.\n",
    "Uses boto3 [get_public_access_block](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/get_public_access_block.html). For k50907905 it is not enabled.\n",
    "\n",
    "*why do we change the name of the function from the boto3 one, isn't it confusing* \n",
    "\n",
    "```list_buckets```: Returns a list of all available buckets in the S3 storage. Uses boto3 [list_buckets](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/list_buckets.html)\n",
    "\n",
    "```delete_bucket```: Returns a boolean value indicating whether the bucket was deleted successfully or not. To be deleted, by default the bucket must be empty. Takes in input the bucket name and a boolean to force the deletion of all objects in the bucket before deleting the bucket. \n",
    "\n",
    "uses [head_bucket](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/head_bucket.html) to determine if a bucket exists. then deletes the bucket using [delete_bucket](https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-bucket.html)\n",
    "\n",
    "If force=True, uses [list_object_versions](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/list_object_versions.html) to list all versioned objects in a bucket and delete them, before deleting the non-versioned objects with [delete_object]()\n",
    "\n",
    "*adding the version id make it so that the deletion is not a new object version?* \n",
    "\n",
    "```delete_file```: Returns a boolean value indicating whether the file was deleted successfully or not. TTakes in input the file_key and its bucket name. Uses [delete_object](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/delete_object.html)\n",
    "\n",
    "\n",
    "```_encode_metadata```: internal method to encode metadata values to handle non-ASCII characters that are not allowed or not safe in URLs. Takes in input a dictionary with metadata and returns the correspondiong url-encoded dictionary. It is an internal method, supposedly it is like that to not use it outside the class/module. It is called by the ```store_file``` method.\n",
    "\n",
    "*why is it an internal method like this? and not a util for instance?* \n",
    "\n",
    "**Issues** \n",
    "- check exception handling of all functions\n",
    "- in 'delete objects from manifest'function check if there where versione objects that have not been deleted. Implement checking for other versions mayube as an option?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf300cd0",
   "metadata": {},
   "source": [
    "# Other to document\n",
    "\n",
    "## razu/\n",
    "\n",
    "the name of the class is also the name of the file(.py)\n",
    "\n",
    "**Tasks**: create MDTO RDF metadata of resources.\n",
    "- `RDFResource`: RDFResource represents an RDF node (either a URIRef or a BlankNode) along with its associated graph.\n",
    "    It provides methods to add properties, handle nested data, and combine graphs.\n",
    "- `MetaResource(RDFResource)`: Generates identifiers via Identifiers class, give context to the resource loading Config and all doirectories paths etc\n",
    "- `StructuredMetaResource(MetaResource)`: StructuredMetaResource extends MetaResource with additional methods to handle structured data. Adds domain specific LDTO metadata\n",
    "- `MetaGraph`: Sets MDTO Namespaces and prefixes\n",
    "- `Identifiers`: Generate uid uri from data in the Config file\n",
    "- `ConnceptResolver`: Resolves URIs for terms from a vocabulary and creates Concept objects\n",
    "\n",
    "\n",
    "- `EDepot`: class and method to interact with S3 storage via boto3 library\n",
    "- `Manifest` and `ManifestEntry`: A class to manage a manifests\n",
    "\n",
    "- `Sip`: manage SIPS\n",
    "\n",
    "[link to repo](https://github.com/Regionaal-Archief-Zuid-Utrecht/razulibs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5063bb07",
   "metadata": {},
   "source": [
    "# Newspaper tooling\n",
    "\n",
    "- `create_sip.py` : The script reads metadata from a SQLite database (db/resources.db) and produces RDF files (default JSON-LD) representing digitized newspaper collections. Depending on a configured OUTPUT_LEVEL, it generates RDF for: COLLECTION level, TITLE level, ISSUE level, PAGE level (default in the script). It also manages copying/moving/ignoring associated image files, and builds an IIIF Manifest via an external Manifest class.\n",
    "\n",
    "scripts:\n",
    "\n",
    "* `collect_rdf.py`:\n",
    "* `index_kranted.py`: script that connects to the triply SPARQL endpoint and to the files location on the NAS, and indexes the combined content into Elasticsearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e00c1",
   "metadata": {},
   "source": [
    "## used softwares by other institutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fbbd52",
   "metadata": {},
   "source": [
    "### [Nijmegen](https://regionaalarchiefnijmegen.nl/over-ons/353-e-depot-1)\n",
    "\n",
    "[Archivematica](https://www.archivematica.org/en/) hosted on Vitec [Memorix BV](https://www.vitec-memorix.com/)\n",
    "\n",
    "- OAIS compliant\n",
    "- open-source\n",
    "\n",
    "Published via [Alantis](https://www.atlantis-erfgoed.nl/?lang=en) by DEVENTit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9943be",
   "metadata": {},
   "source": [
    "### KB\n",
    "\n",
    "[Rosetta](https://knowledge.exlibrisgroup.com/Rosetta/Product_Documentation/Rosetta_Overview_Guide/003_Consortial_Administration) software by Ex-Libris\n",
    "\n",
    "- OAI data export for external use (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33552d71",
   "metadata": {},
   "source": [
    "### Zuphten \n",
    "Also used as CoreTrustSeal reference by Bram\n",
    "\n",
    "They also use **Archivematica** hosted by Vitec Memorix. It consists of a test environment and a production environment. \n",
    "\n",
    "<q>Within the production environment, there are three separate locations to store the digital archive objects of the three\n",
    "municipalities whose archives are kept and managed by the RAZ. During the ingest process, the e-curator manually selects the appropriate storage\n",
    "location. All locations are managed in the same way, as their contents need to be preserved permanently and are subject to the same legal requirements.\n",
    "Any private archives the RAZ may receive in the future will also be stored in the e-depot permanently and will therefore be managed in the same way as\n",
    "the archived public records.</q>\n",
    "\n",
    "Archivematica:\n",
    "- based on OAIS model\n",
    "- scalable \n",
    "- data centres use the following standards and references: ISO 9001, 14001, 27001, 50001. OHSAS 18001. ISAE 3402. AMS-IX. PCI-DSS\n",
    "- vitec memorix monitors changes\n",
    "- **Replication of Data** The stored data is continuously replicated from the primary site to the fail-over site. This ensures that all data is secured at geographically separate sites\n",
    "- Instead of creating back-ups, Vitec Memorix works with snapshots. This technology is unique for the file system used by Vitec Memorix (ZFS). Snapshots\n",
    "make it possible to go back in time to a snapshot (increments of 1 hour) up to the last saved back-up. This is also possible in the case of, for example,\n",
    "encryption (ransomware cybercrime, etc.). It is an extremely advanced method of security and certainty without the administrative and storage burden of\n",
    "(incremental) back-ups\n",
    "- Specific servers are set up for different tasks in the architecture. For example, image servers, streaming servers, file conversion servers, application\n",
    "servers, etc. The division of tasks makes it easy to deploy extra strength per task if necessary\n",
    "\n",
    "\n",
    "#### Vitec Memorix has ISO-27001 certificate for security\n",
    "• Logical access control – using strong passwords that are changed very regularly\n",
    "• Screen locks\n",
    "• Physical measures for access security\n",
    "• Securing network connection using TLS technology (formerly SSL)\n",
    "• Purpose-specific access to personal data\n",
    "• Checking assigned authorities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9d856",
   "metadata": {},
   "source": [
    "### Nationaal archief\n",
    "\n",
    "Security: uses the [Central Government Information Security Baseline (BIR:2012)](https://www.nldigitalgovernment.nl/overview/government-information-security-baseline/) as the basic level for the information security of the e-Depot.\n",
    "\n",
    "They use [Preservica](https://preservica.com/)\n",
    "\n",
    "The specific storage layer linked to the storage adapters is built by using commercial off-the-shelf servers. The disk\n",
    "cabinets connected to servers provide the servers with [RAID 6 volumes](https://en.wikipedia.org/wiki/Standard_RAID_levels). On top of those volumes we created a software-based network\n",
    "attached replicated and distributed scale-out file system with a global name space (based on [Red Hat Gluster Storage](https://www.redhat.com/en/technologies/storage/gluster)). In theory, this provides our e-depot with unlimited storage\n",
    "space.\n",
    "All archival data is replicated and backed up at a remote site, approximately 200 kilometres away (ODC Noord, see\n",
    "Requirement 0, Outsource partners). Our backup strategy uses ‘incremental-forever backup’: if there is a full backup, the\n",
    "delta can be backed-up within one week ([Recovery Time Objective](https://en.wikipedia.org/wiki/Recovery_time_objective))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d4842",
   "metadata": {},
   "source": [
    "### [Alkmaar](https://www.regionaalarchiefalkmaar.nl/over-ons/e-depot)\n",
    "\n",
    "Also Archivematica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768dfe2a",
   "metadata": {},
   "source": [
    "### Utrecht Archief\n",
    "\n",
    "[Pilot project with the U municipality](https://hetutrechtsarchief.nl/vakgenoten/e-depot/wat-is-een-e-depot?view=article&id=35&catid=19)\n",
    "\n",
    "[Impact analysis (with national Archief)](https://www.nationaalarchief.nl/archiveren/nieuws/impactanalyse-goede-voorbereiding-op-digitale-archivering)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
